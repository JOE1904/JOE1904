{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwCXQcPsg73w1ooX60ZNww",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JOE1904/JOE1904/blob/main/Final_Code_Architecture(Fair).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "NLkwvGavWokm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "DWPwwoqFWqbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "trOCksvRWsfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "XF5G0WiUYMo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "KUEhO5H-YPdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api"
      ],
      "metadata": {
        "id": "YjkmAGL7Y12B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "F4lAVisSWiBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP: 1A  Converting PDF To Text or Taking in Text Files:**"
      ],
      "metadata": {
        "id": "sXBzl7nkSHxE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buOfYmgdRw0q"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_file = open('example.pdf', 'rb')\n",
        "\n",
        "# Create a PDF reader object\n",
        "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
        "\n",
        "# Initialize an empty string to store the text\n",
        "text = \"\"\n",
        "\n",
        "# Loop through each page and extract text\n",
        "for page_number in range(pdf_reader.getNumPages()):\n",
        "    page = pdf_reader.getPage(page_number)\n",
        "    text += page.extractText()\n",
        "\n",
        "# Close the PDF file\n",
        "pdf_file.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP : 1B Converting PDF To Text or Operating Directly on Text Files:**"
      ],
      "metadata": {
        "id": "eng8pagWS_fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting PDF To Text File:\n",
        "with open('output.txt', 'w') as file:\n",
        "    # Write your text to the file\n",
        "    file.write(text)\n",
        "\n",
        "# The file is automatically closed when the \"with\" block is exited\n",
        "print(\"Text has been written to 'output.txt'.\")\n",
        "\n",
        "# Working on the Text File:\n",
        "file_path =  \"/content/sample_data/data_sci.txt\" # Replace '/content/sample_data/data_sci.txt' with the path to your text file\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the entire file content into a string\n",
        "    file_content = file.read()\n"
      ],
      "metadata": {
        "id": "uUhco6hrTMAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP: 1C Text-Preprocessing (The Necessary Things)**\n",
        "\n",
        "**The following steps will be carried out in this block of code:** <br>\n",
        "  - The text will iterate through a loop where all the tokens excluding numerical values will be taken out.\n",
        "  - The words/tokens taken out will undergo the Name-recognition Step.\n",
        "  - The text/corpus will remain the same. This step is done for the Highlight step in the website."
      ],
      "metadata": {
        "id": "wwLy5o5-UHWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numerical values, punctuation, and special characters:\n",
        "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', re.sub(r'\\d+', '', file_content))\n",
        "\n",
        "# Remove all numerical values from the text:\n",
        "text_without_numbers = re.sub(r'\\d+', '', cleaned_text)"
      ],
      "metadata": {
        "id": "PSipttQSVUip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP: 2 Name- Entity Recognition:**"
      ],
      "metadata": {
        "id": "3L-h25XPSV1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "# Putting corpus into pipeline for entity recognition:\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Creating A DatFrame for the Name- Entities Created :\n",
        "data = [(result['entity'], result['word']) for result in ner_results]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data, columns=['Entity Type', 'Word'])\n",
        "df"
      ],
      "metadata": {
        "id": "tO-Acrz8Sfbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP:3 Generating/ Web-Scraping of each word from Wikipedia or Relevant Data Source :**"
      ],
      "metadata": {
        "id": "mY-EvRsSXtyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                      # OPTION 1: Web-Scraping from Wikipedia\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the Wikipedia page you want to scrape\n",
        "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
        "\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content of the page using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find the element containing the introduction paragraph\n",
        "intro_paragraph = soup.find(\"p\")\n",
        "\n",
        "# Extract the text from the paragraph element\n",
        "if intro_paragraph:\n",
        "    intro_text = intro_paragraph.get_text()\n",
        "    print(intro_text)\n",
        "else:\n",
        "    print(\"Introduction paragraph not found on the page.\")\n",
        "\n",
        "\n",
        "                                                      # OPTION - 2 From Wikipedia Library in Python:\n",
        "\n",
        "import wikipediaapi\n",
        "\n",
        "# Specify the language for Wikipedia (e.g., 'en' for English)\n",
        "wiki_lang = wikipediaapi.Wikipedia('en')\n",
        "\n",
        "# Specify the keyword you want to search for:\n",
        "\n",
        "for keyword in df[\"tokens\"]:\n",
        "  search_keyword = keyword\n",
        "\n",
        "  # Get the page object for the search keyword\n",
        "  page = wiki_lang.page(search_keyword)\n",
        "\n",
        "  # Check if the page exists and has an introduction section\n",
        "  if page.exists():\n",
        "    introduction_section = page.sections[0]  # The first section is usually the introduction\n",
        "\n",
        "    # Extract the text of the introduction paragraph\n",
        "    intro_text = introduction_section.text\n",
        "\n",
        "    df[\"Intro_text\"] = intro_text\n",
        "  else:\n",
        "    print(\"Page not found on Wikipedia for the given keyword.\")"
      ],
      "metadata": {
        "id": "47gaDDAqX78w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CwPrw7LOYLOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEP: 4 Text Summarizer for the corpus for each Word:**"
      ],
      "metadata": {
        "id": "UBN_MWKLXhxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: T5F Large"
      ],
      "metadata": {
        "id": "MnawBywnbied"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained T5 model and tokenizer:\n",
        "model_name = \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "for intro_text in df[\"Intro_text\"]:\n",
        "  inputs = tokenizer.encode(\"summarize: \" + text_to_summarize, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "  summary_ids = model.generate(inputs, max_length=120, min_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode and print the summary\n",
        "  summary_tf_small = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  df[\"Summary_tf-small\"] = summary"
      ],
      "metadata": {
        "id": "3OQZvyqYXsfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: BERT or RoBERTa"
      ],
      "metadata": {
        "id": "QrGt8D_Nbr83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "moFVIrEKb0F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Model: GPT-2 Summarizer:"
      ],
      "metadata": {
        "id": "suZkC7oGb2rn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rN5y-BMbzQ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}